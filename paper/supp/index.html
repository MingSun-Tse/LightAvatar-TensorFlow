<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta name="keywords" content="Neural Light Field, distillation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LightAvatar: Efficient </title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./webpage/static/css/bulma.min.css">
  <link rel="stylesheet" href="./webpage/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./webpage/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./webpage/static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./webpage/static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./webpage/static/js/fontawesome.all.min.js"></script>
  <script src="./webpage/static/js/bulma-carousel.min.js"></script>
  <script src="./webpage/static/js/bulma-slider.min.js"></script>
  <script src="./webpage/static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-1 publication-title">LightAvatar: Efficient Head Avatar as Dynamic Neural Light Field</h2>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div align="center"> <img src="webpage/static/images/teaser.png" width="800px"> </div>
        <div class="content has-text-justified">
Fig. 1: (a) Overview comparison between existing neural head avatars (top) vs. our
LightAvatar (down) â€“ a brand-new framework to build efficient 3D head avatars based
on neural light field. LightAvatar features a simple and uniform design, which takes
expression code and camera pose as input, renders the RGB via a single network forward pass, running at <b>174.1 FPS</b> (on a RTX3090 GPU) with image quality improved.
(b) FPS and LPIPS comparison of recent top-performing (fast) avatars. Our method
achieves much faster rendering speed with better LPIPS than the counterparts.
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
Recent works have shown that neural radiance fields (NeRFs) on top of parametric models have reached SOTA quality to build photo-realistic head avatars from a monocular video. However, one major limitation of the NeRF-based avatars is the slow rendering speed due to the point sampling operation of NeRF, preventing them from broader utility on resource-constrained devices. We introduce LightAvatar, the first head avatar model based on neural light fields. LightAvatar renders the target image from 3DMM parameters and a camera pose via a single network forward pass, without using mesh or volume rendering. The proposed approach, while being conceptually appealing, poses a significant challenge towards real-time efficiency and training stability. To resolve them, we introduce several dedicated network designs to obtain proper representation for the NeLF model and maintain a low FLOPs budget. Meanwhile, we introduce a distillation-based training strategy that uses a pretrained avatar model as teacher to synthesize abundant pseudo data for training. A warping field network is introduced to correct the fitting error in the real data so that the model can learn better. Extensive experiments suggest that our method can achieve the new SOTA image quality quantitatively or qualitatively, while being significantly faster than the counterparts, reporting 174.1 FPS (512x512 resolution) on a consumer-grade GPU (RTX3090) with no customized optimization.
          </div>
        </div>
      </div>

    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">

          <!-- NeRF results: Blender -->
          <h2 class="title is-3">1. Video Comparison on 5 Subjects (Tab. 1 / Fig. 3 in the paper) </h2>
          <p>TL'DR: We achieve <b>comparable or slightly better</b> video quality than the prior SOTA method MonoAvatar, while being over <b>300x faster</b> (see Tab. 3 in the paper), by using neural light field as an efficient representation for head avatars.</p>
            <p></p><p></p>
            <p></p><p></p>
            
          <div class="content has-text-centered">
            <p><i><b>Subject 1</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/subject1.mp4" type="video/mp4" />
            </video>
            <p></p><p></p>
          </div>

          <div class="content has-text-centered">
            <p><i><b>Subject 2</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/subject2.mp4" type="video/mp4" />
            </video>
            <p></p><p></p>
          </div>

          <div class="content has-text-centered">
            <p><i><b>Subject 3</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/subject3.mp4" type="video/mp4" />
            </video>
            <p></p><p></p>
          </div>

          <div class="content has-text-centered">
            <p><i><b>Subject 4</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/subject4.mp4" type="video/mp4" />
            </video>
            <p></p><p></p>
          </div>

          <h2 class="title is-3">2. Video Comparison with Fast Avatars (Tab. 2 / Fig. 4 in the paper) </h2>
          <p>TL'DR: We achieve <b>significantly better</b> video quality than the prior fast avatars, while being <b>much faster</b>, too (see Tab. 3 in the paper), by using neural light field as an efficient representation for head avatars.</p>
            <p></p><p></p>
            <p></p><p></p>
            
          <div class="content has-text-centered">
            <p><i><b>Subject 8</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/subject14.mp4" type="video/mp4" />
            </video>
            <p></p><p></p>
          </div>

          <div class="content has-text-centered">
            <p><i><b>Subject 9</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/subject15.mp4" type="video/mp4" />
            </video>
            <p></p><p></p>
          </div>

          <div class="content has-text-centered">
            <p><i><b>Subject 10</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/subject16.mp4" type="video/mp4" />
            </video>
            <p></p><p></p>
          </div>

          <div class="content has-text-centered">
            <p><i><b>Subject 11</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/subject18.mp4" type="video/mp4" />
            </video>
            <p></p><p></p>
          </div>

          <div class="content has-text-centered">
            <p><i><b>Subject 12</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/subject19.mp4" type="video/mp4" />
            </video>
            <p></p><p></p>
          </div>

            <h2 class="title is-3">3. Video comparison: Joint vs. separate modeling in our method</h2>
            <p>TL'DR: The proposed separate modeling scheme achieves much better temporal consistency in our method.</p>
            <p></p><p></p>
            <p></p><p></p>
          <div class="content has-text-centered">
            <p><i><b>Subject 13 (with shoulder)</b></i></p>
            <video class="video-fluid w-100" controls autoplay loop muted>
              <source src="webpage/static/videos/with_shoulder.mp4" type="video/mp4" />
            </video>
          </div>
            
        </div>
      </div>
    </div>
  </section>


  <footer class="footer">
    <div align="center" class="container">
      <div class="columns is-centered">
        <div class="content">
          This website refers to <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a> and <a href="https://snap-research.github.io/R2L/">R2L</a>. We sincerely thank them for the contributions!
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
